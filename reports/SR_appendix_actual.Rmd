---
title: "Systematic Review Appendix materials"
author: "2509920F"
date: "`sa"
output: 
  officedown::rdocx_document:
     reference_docx: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/SR_Template.docx'
     tables:
      style: Table
      layout: autofit
      width: 1.0
      caption:
       style: Table Caption
       pre: 'Table '
       sep: ': '
      conditional:
       first_row: true
       first_column: false
       last_row: false
       last_column: false
       no_hband: false
       no_vband: true
bibliography: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/SR_biblio.bib'
csl: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/non-dclin/5_Code/R library/csl/neuropsychology.csl'
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
require("knitr")
# opts_knit$set(root.dir = "..") #  I put this in to raise up the working directory from the reports folder (where the rmd lives) to one level above
```


```{r, launch_scripts, echo=FALSE, fig.show='hide', include=TRUE, warning=FALSE}
# This is the cleaning and pre-processing described above.
# source("./scripts/SBEP_staff_script_q3.R", print.eval = FALSE, echo = TRUE, keep.source = TRUE)
 # source("./scripts/0_SR_libraries_and_load.R", echo = FALSE)



 source("C:\\Users\\Alexander Fradera\\OneDrive - University of Glasgow\\DClin\\Deliverables\\Systematic Review\\Writeup\\DCLIN_SR_git\\scripts\\model_making.R", echo = FALSE)


```

# Supplementary exploratory information

```{r, violins, echo=F}
ggplot(dfm_mod, aes(x=1, y=yi)) + geom_violin() +
  scale_x_continuous(breaks=NULL) +
  theme(axis.title.x = element_blank()) + ylab("effect size") + ggtitle("Violin plot of frequencies of effect sizes across dataset")
```
```{r, mean_distro, echo = F}
a2 <- dfm2 %>%
  ggplot(mapping = aes(x=cognitive_mean_treat, fill = cognitive_name)) + geom_histogram(binwidth = 2, alpha = 0.4)  + xlab("Pain groups") + theme(legend.position = "bottom") + xlim(10,100) +labs(fill ="Cognitive screen")


a1 <- dfm2 %>%
  ggplot(mapping = aes(x=cognitive_mean_cont, fill = cognitive_name)) + geom_histogram(position = "identity", binwidth = 2, alpha = 0.4)  + xlab("Control groups")  + guides(fill = FALSE) + xlim(10,100) + ggtitle("Histogram of raw cognitive screen scores across dataset") 
a1 / a2
# , fill = "wheat", color = "black"
```

```{r captions, include=FALSE, eval=TRUE}


set_flextable_defaults(font.family = "Calibri (Body)",
                       font.size = 9,  
                       font.color = "black",
                       # italic = FALSE,
                       digits = 0, 
                       border.color = "#000000",
                       padding.bottom = 1,
                       padding.top = 1,
                       padding.left = 3,
                       padding.right = 1)


fp_text_default(
  color = "#000000",
  bold = FALSE,
  italic = FALSE,
  underlined = FALSE,
    cs.family = NULL,
  eastasia.family = NULL,
  hansi.family = NULL,
  vertical.align = "baseline",
  shading.color = "transparent"
)

library(captioner)
figure_nums <- captioner(prefix = "Fig.")
table_nums <- captioner(prefix = "Table")
sup_figure_nums <- captioner(prefix = "Supplementary Fig.")
sup_table_nums <- captioner(prefix = "", levels = 0)

#set the table caption styling
knitr::opts_chunk$set(tab.cap.pre = "Supplementary Table ")

# Supplementary figure definition (keep same as in Appendix for order)



sup_figure_nums(name="just_testing", caption = "**I'm just testing") 
sup_figure_nums(name="just_testing2", caption = "I'm still just testing") 
sup_table_nums(name="diagnoses", caption = "") 




```


# Approach to dealing with multiple comparisons

Some studies offered more than one relevant comparison leading to extraction of multiple effect sizes in these cases. The table below summarises which studies provided multiple estimates.
``` {r, repeaters, echo = F}



wide_repeats <-repeats %>%
pivot_wider( names_from = cognitive_name,
                             values_from = sample_treat)


wide_repeats2 <- wide_repeats %>%
  ungroup %>%
  select(!c(unique_id,study_id, comp,sample_cont)) 


wide_repeats3 <- wide_repeats2 %>%
  mutate(grouper = c(11, 12, 1, 2, 13, 14, 5, 6, 7, 8, 9, 10, 19, 20, 15, 16, 17, 18, 3, 4



)) %>%
  arrange(grouper) %>% relocate(author_final, MMSE)




authors <- (wide_repeats3$author_final)
condition  <- (wide_repeats3$MMSE)


repeattab <- flextable(select(wide_repeats3, !grouper))
repeattab <- set_table_properties(repeattab, layout = "autofit")
repeattab <- set_header_labels(repeattab, 
                 author_final = "Authors")
repeattab

```

In four studies, multiple groups were compared against the same set of healthy controls: `r authors[1]` for `r condition[1]` and `r condition[2]`, and `r authors[3]` for `r condition[3]`, and `r condition[4]`, with `r authors[5]` involving groups for `r condition[5]`,`r condition[6]` and `r condition[7]`. As the control group data was used repeatedly for these comparisons, the effect size estimates are correlated (due to the correlation of the sampling error of these estimates).

In addition, `r authors[5]` used multiple screens, the  MMSE and Test Your Memory. The remaining multiple comparisons involved a single control group and patient group for both the MMSE and MoCA - `r authors[11]` and `r authors[18]`.

One further study, `r authors[19]` involved two separate pain groups each with a matched control group. The factor differentiating these pairs was a variable not relevant to this review (whether the individuals acted as caregivers) and the decision was made that this source of data would be better incorporated into the model as a single pair; accordingly these were merged using the functions below.



``` {r, combining, echo = T}
mean_function <- function(m1,m2,n1=size1,n2=size2){
                          ((n1*m1) + (n2*m2))/(n1+n2) }

sd_function <- function(m1,m2,s1,s2,n1=size1,n2=size2){
                         sqrt(((n1-1)*s1^2 + (n2-1)*s2^2 + n1 * n2 / (n1 + n2) * (m1^2 + m2^2 - 2 * m1 * m2)) / (n1 + n2 -1)) }
```

## Correcting for multiple comparisons




```{r, different_effects, eval = F, echo = F}
# Computing effect sizes for two different approaches  - from esc function (es) and metafor(yi) - very very similar.

checkin <- dfm_mod %>%
select(study_id, n_cont,n_treat, cognitive_mean_cont, cognitive_mean_treat, cognitive_sd_cont, cognitive_sd_treat,  es,yi)   %>%
  slice(11:19)

flextable(checkin) %>%
  colformat_double(digits=4, j = (8:9) )

```

To deal with this a Correlated Hierarchical Effects (CHE) model was utilised. As part of this workflow, a variance-covariance matrix was computed across all the comparisons (dimensions `r length(unique(dfm_mod$unique_id))` * `r length(unique(dfm_mod$unique_id))`) using the vcalc() function from the *metafor* package. In such a matrix, the diagonal (identity) composed of the sampling variance of each study (eg, element [6,6] would contain the sampling variance for comparison 6). If that comparison is unrelated to others, only this element will be used to stand-in for the  study sampling variance (meaning the study sampling variance remains the study sampling variance). When groups are related the vcalc() process will produce covariance estimates of the two (sampling variances) at the appropriate positions (e.g., if studies 7 and 8 share a control group, matrix elements [7,8] and [8,7] will incorporate these covariance estimates). This is further shaped by a correlation matrix based on published relationships between cognitive screens: MoCA with MMSE [@Nasreddine2005], MMSE with ACE (ACE-III) [@MatiasGuiu2017], MMSE with TYM [@Zande2017]. Other relationships were estimated but are not actually required as these relationships are not relevant for these studies. 

```{r, callingR, echo = F}
R
```

For eg Liao et al. (2018), with one MMSE and one MoCA comparison on identical groups, the matrix elements are:

```{r, cov}
 V[dfm_mod$study_num == 121, dfm_mod$study_num == 121]
```

and the underlying correlation matrix is, as expected, 
```{r, cor}
 cov2cor(V[dfm_mod$study_num == 121, dfm_mod$study_num == 121])
```

Model calculation then draws on this V matrix to inform the final weighting of comparisons; the effect is a downweighting of estimates from studies with multiple comparisons. See the comparable process documented at https://wviechtb.github.io/metadat/reference/dat.knapp2017.html. Results from this process are reported in the the results of chapter 1.

# RVE calculation


An additional model was conducted using a robust variation estimation (RVE) approach introduced as a final step to the CHE workflow described above. This introduces a Sandwich estimator that can be superior in estimating standard errors and thus the confidence intervals around derived effects. This can be beneficial in clustered datasets particularly when the number of clusters is small. In our case, the model outputs were almost identical to the original multi-level approach, and was therefore not pursued further.

*Original approach:*

```{r che}
che.model

```

*RVE approach:*

```{r vsrve}

rve.model
```



``` {r rveresids, eval = F, echo =F}
resids_sav <- as_tibble(residuals(rve.model))
ggplot(data = resids_sav, mapping = aes(x=1, y=value)) + geom_violin() +
  scale_x_continuous(breaks=NULL) +
  theme(axis.title.x = element_blank()) + ylab("residuals")
```

# References

