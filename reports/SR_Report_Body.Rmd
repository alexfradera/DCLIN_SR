

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
require("knitr")
# opts_knit$set(root.dir = "..") #  I put this in to raise up the working directory from the reports folder (where the rmd lives) to one level above
```


```{r launch_scripts, echo=FALSE, fig.show='hide', include=TRUE, warning=FALSE}
# This is the cleaning and pre-processing described above.
# source("./scripts/SBEP_staff_script_q3.R", print.eval = FALSE, echo = TRUE, keep.source = TRUE)
 # source("./scripts/0_SR_libraries_and_load.R", echo = FALSE)


 source("C:\\Users\\Alexander Fradera\\OneDrive - University of Glasgow\\DClin\\Deliverables\\Systematic Review\\Writeup\\DCLIN_SR_git\\scripts\\model_making.R", echo = FALSE)


# citation("metafor") etc

```




# Abstract




# Introduction


Cognitive screening tools are measures designed to detect cognitive impairment through brief means. These either target one highly predictive ability or are addressed at a set of core domains (language, memory, orientation) using a minimal set of items for each. Measures considered cognitive screens are typically those that can be completed within 20 minutes [@Cullen2007] and are not an alternative to fuller neuropsychological test batteries, which are more comprehensive and sensitive. Cognitive screening occurs in a wide variety of contexts such as patients with brain tumours, psychiatric disorders, or traumatic brain injuries [@RoebuckSpencer2017]; however screening for dementia is a rationale behind the construction of many . 

Cognitive screens are not intended to be diagnostic, although low scores that corroborate clear deficits found in a clinical examination and interview may be considered sufficient without further cognitive investigation. Outside of this context, cognitive screen performance contributes to the decision whether more investigation is required. Key here is that cognitive screens are sufficiently sensitive - correctly identifying when followup is warranted, to maximise early diagnosis - and specific - avoiding putting people on an investigative pathway when this is not necessary [@Cullen2007].


These concerns are relevant when a population lives with a cognition-affecting condition that is not the disease being screened for, such as experiencing chronic pain. Pain is known to affect performance on focused neuropsychologicalcognitive tests and batteries  (see xx for a review). This is likely to reflect a combination of depletion of resources, distracting effects of pain symptoms disrupting attention, and in the case of chronic pain possible longer-term changes to the nervous system either due to sustained pain or the condition that produces it. As chronic pain is more prevalent with aging, it is important to understand whether the experience impacts cognition sufficiently to result in alterations of cognitive screen responding.




<!-- ============================  -->

# Method

The study followed the PRISMA guidelines for reporting SRs and MAs [@Moher2009].

 The protocol of the study was registered with the International Prospective Register of Systematic Reviews (PROSPERO) [registration number: CRD 42021272835] and published elsewhere (EG ZZZZ Martín-Gómez et al.,
2020).

## Search strategy

Searches were conducted through the bibliographic databases Embase (1947-present), Medline (1946-present), and PsychInfo, as a mapping of articles identified in preliminary searches showed saturation by the use of these databases. OpenGrey (System for Information on Grey Literature in Europe) was additionally searched ??? Currently down.


The search was performed using the PICO model. It used medical headings and keywords associated with pain and cognitive screening instruments.Searches were initially piloted in PsychInfo before adaptation for use in the other databases. The search strategy for each database can be found in Appendix XXXX. PROSPERO and Epistimonikos were searched for similar ongoing or recently completed SRs.



## Eligibility criteria

The review focused on primary research that involved a participant group experiencing chronic pain and a control group without that experience. 
Studies had to include participants completion of a cognitive screen. Definitions of cognitive screen are varied and the subject of a number of previous systematic reviews [eg @Ashford2008, @Cullen2007], and the decision was made to utilise a practitioner definition provided in @AlzheimersSociety2013 which reports nine screens agreed by UK clinicians to be the focus of common clinical practice. Studies using different editions and language variants of these screens were also eligible. 

Study design was not strictly defined, and could include cross-sectional designs as well as experimental designs including those introducing treatment such as medication, as long as cognitive screening information was available for both groups, and reporting was not clearly confounded by the impact of the treatment. This was rarely the case as screen information tended to be collected at baseline for these studies. 

## Study selection
After acquiring search results and removal of duplicates, two initial co-review stages were taken by reviewers 1 (AF) and 2 (JM): firstly to calibrate the eligibility checklist on 10 results and agree refinements, and then to independently screen 120 titles and abstracts against inclusion criteria, and then discuss discrepancies in judgment until reaching consensus on all cases, consulting with a third author where necessary.

Following this step the full texts of retained studies were reviewed starting with an eligibility calibration on 2 papers, followed by independent screening of 20 further full texts by the same two reviewers, addressing discrepancies in a similar manner. Reviewer 1 then completed the full text review on all remaining results. Authors were contacted when full articles are unavailable (n=1).

## Data extraction

Relevant data included type of pain condition, participant details,  measures of pain and mood and scores on the cognitive screening test, as well as whether the test was key or incidental to the study (e.g. a baseline measure). The cognitive screen data extracted was in the form of reported means and standard deviations; where data was presented in the form of median and inter-quartile range, this was converted into estimated mean and standard deviation using the estmeansd r package using the Box–Cox method described in @McGrath2020.

Data was extracted by A.F, with J.M performing a check to ensure accurate extraction on 5 consecutive papers, which was achieved after 8 papers.

## Statistical analyses

Cognitive screen means and standard deviations were used to compute standardised mean differences in the form of Hedge's g using the approach described by @Hedges1985. Scores reflect the size of effect for membership of the chronic pain group versus control with a larger score reflecting greater impairment in the chronic pain group.

A number of studies reported more than one comparison of cognitive screen scores that fit the review criteria due to multiple chronic pain groups compared to one or multiple control groups or two cognitive screens administered across participants.  In most instances this led to generating multiple standard mean differences per study, leading to interdependency between outcomes. This was addressed by introducing to the model a  covariance matrix estimating these interdependencies, including information about the relationships between cognitive screen scores. In one case, it was judged that groups could be more appropriately merged to produce a single standardised mean difference. 
This is described in appendix XXXXX.

The metafor package and 
[@Viechtbauer2010]

In order to examine potential sources of variance between studies, a meta-regression approach was used to estimate the influence of potential moderators on the overall effect size. These were cognitive screen, chronic pain classification, age and disease duration.


## Risk of bias 
 <!-- this is not found in every one  -->

## Assessment of publication bias ??


Maybe only for those for which screening was key to the study

## Data analysis

## Quality of evidence / Quality assessment tool

Study quality was assessed using the JBI Critical Appraisal Checklist for Analytical Cross Sectional Studies (REF). This scale rates studies on eight criteria such as inclusion criteria, description of study, outcome measurement and strategies to deal with confounding factors. The checklist includes items that required some translation to relevance for this review, which was supplied to both reviewers as supplementary guidance that can be found in appendix.





# Results


## Study selection

``` {r, echo = F}
library("flextable")
library("ftExtra")
library("dplyr")
library("officer")
library(openxlsx)

set_flextable_defaults(
  font.size = 10, theme_fun = theme_vanilla,
  padding = 6,
  background.color = "#EFEFEF")


overview <- read.xlsx("C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/data/study_overview_sheet.xlsx")
overview <- overview %>%
  mutate(biblio_comp = paste0("@",biblio))

pres_overview <- select(overview, study_id, biblio_comp)

pres_dat <- dfm_e %>%
  select(study_id, sample_treat, n_treat, age_mean_treat, age_sd_treat,  n_cont, matching_education, cognitive_name )

test <- left_join(pres_dat,pres_overview, by="study_id")
test <- test %>%
  arrange(biblio_comp) %>%
  relocate(biblio_comp) %>%
  select(!study_id)

test <- test %>%
  mutate(age_mean_treat = round(age_mean_treat, 2),
         age_sd_treat = round(age_sd_treat, 2),
         age_sum_treat = paste0(age_mean_treat," (",age_sd_treat,")")
         )


tab1 <- test %>%
  select(!c(age_mean_treat,age_sd_treat)) %>%
  arrange(biblio_comp) %>%
  relocate(biblio_comp) %>%
  flextable() %>%
  colformat_md() %>%
  autofit()
tab1 <- set_table_properties(tab1, layout = "autofit")
set_header_labels(tab1, 
                  biblio_comp = "Authors", 
                  sample_treat = "Pain group",
                  n_treat = "Patient group size",
                  n_cont = "Control group size",
                  cognitive_name = "Screen",
                  age_sum_treat = "Patient age; mean (sd)")
                  

# put this outside the block to work <!---BLOCK_LANDSCAPE_START---> <!---BLOCK_LANDSCAPE_STOP--->
# as per https://ardata-fr.github.io/officeverse/officedown-for-word.html#insert-sections
```






## Study characteristics

There were `r sum(dfm_mod$cognitive_focus=="key")` comparisons where the screen was critical to the study focus and `r sum(dfm_mod$cognitive_focus=="incidental")` where the screen was used incidentally.

## Participants 
<!-- this is not found in every one  -->

NEED A 


## Type of screen
<!-- this is not found in every one  -->

## Risk of bias of the included studies

The evaluation of confounders reported above does not include mood or anxiety. These two features are considered prevalent and co-occurring symptoms with pain, forming part of a symptom cluster [@Davis2016] together. Accordingly, mood was significantly higher for the pain group in ` R CODE ` of the studies, with `R CODE` not reporting mood and only ` x` explicitly matching. Anxiety was rated in only `R CODE` of the studies and was significantly different in `R CODE`.

## Impact of chronic pain upon cognitive screen performance




Cognitive screen score standard mean differences within the dataset ranged from `r round(min(dfm_mod$yiN),2)` to `r round(max(dfm_mod$yiN),2)` as shown in the histogram below.

``` {r, histos, echo = F}
#dfm_mod %>%
#  ggplot(mapping = aes(x=yi)) + geom_histogram(binwidth = .2, fill = "wheat", color = "black")  + xlab("Standardised Mean Difference")

dfm_mod %>%
  ggplot(mapping = aes(x=yiN)) + geom_histogram(binwidth = .2, fill = "wheat", color = "black")  + xlab("Standardised Mean Difference")
```


The average standard mean difference derived from the model was `r format(round(study_outputs$pred, 2), nsmall = 2)` with the 95% confidence interval ranging from `r format(round(study_outputs$ci.lb, 2), nsmall = 2)` to `r format(round(study_outputs$ci.ub, 2), nsmall = 2)`.

Analysis suggested that the true effects were heterogeneous (Q = `r round(che.model_N$QE,2)`, p < 0.0001). This was chiefly due to differences in true effects of studies (`r round(che.model_N$sigma2[1],2)`) with small levels of within-study heterogeneity (`r round(che.model_N$sigma2[2],2)`).



```{r, violins, echo=F}
ggplot(dfm_mod, aes(x=1, y=yi)) + geom_violin() +
  scale_x_continuous(breaks=NULL) +
  theme(axis.title.x = element_blank()) + ylab("effect size")
```




``` {r, forester, echo = F}

forest(che.model_N, slab=author_final, annotate=TRUE, addfit=TRUE, addpred=FALSE, at= seq(-2,5, by =1), xlim=c(-6,8),
       showweights=FALSE, header=TRUE, ilab = cbind.data.frame(sample_treat_cat), ilab.xpos = -3, ilab.pos = 4, order = sample_treat_cat)  # the cbind.data.frame is to stop factor returning as integers
```




``` {r, forester2, echo = F}

forest(che.model_N, slab=author_final, annotate=TRUE, addfit=TRUE, addpred=FALSE, at= seq(-2,5, by =1), xlim=c(-6,8),
       showweights=FALSE, header=TRUE, ilab = cbind.data.frame(sample_treat_cat), ilab.xpos = -3, ilab.pos = 4, order = "obs")  # the cbind.data.frame is to stop factor returning as integers
```

``` {r, forester2, echo = F}

che.model_N_small <- che.model_N

che.model_N_small$vi.f <- che.model_N_small$vi.f[1:25]


forest(che.model_N_small, slab=author_final, annotate=TRUE, addfit=TRUE, addpred=FALSE, at= seq(-2,5, by =1), xlim=c(-6,8),
       showweights=FALSE, header=TRUE, ilab = cbind.data.frame(sample_treat_cat), ilab.xpos = -3, ilab.pos = 4, order = sample_treat_cat)

```



## Publication bias


``` {r, funnelling, echo = F}
 dfm_key<- dfm_mod %>%
    filter(cognitive_focus == "key")

res_key <- rma(yi, vi, data=dfm_key)
res_tri <- trimfill(res_key)
funnel(res_tri, legend= TRUE, main = "Funnel Plot (focused studies)")

# what about without outlier?
dfm_key_out <- filter(dfm_mod, yi <3)

res_key_out <- rma(yi, vi, data=dfm_key_out)
res_tri_out <- trimfill(res_key_out)
funnel(res_tri_out, legend= TRUE, main = "Funnel Plot (focused studies) - no outlier") 

```

## Subgroup analysis, meta-regression and stratified meta-analysis

## Confounds



## Quality of evidence


<!-- ============================  -->

# Discussion
<!-- some do not break down this way -->

## Summary of findings

## Comparison with previous research

## Meaning and implications

## Strengths and limitations

## Future research

 



## Patient data




# Results



