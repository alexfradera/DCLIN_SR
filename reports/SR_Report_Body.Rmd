---
title: "Does the presence of chronic pain affect scores on cognitive screening tests/brief cognitive measures? A Systematic Review"
author: "2509920F"
date: ""
output: 
  officedown::rdocx_document:
     reference_docx: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/SR_Template.docx'
     tables:
      style: Table
      layout: autofit
      width: 1.0
      caption:
       style: Table Caption
       pre: 'Table '
       sep: ': '
      conditional:
       first_row: true
       first_column: false
       last_row: false
       last_column: false
       no_hband: false
       no_vband: true
bibliography: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/SR_biblio.bib'
csl: 'C:/Users/Alexander Fradera/OneDrive - University of Glasgow/non-dclin/5_Code/R library/csl/neuropsychology.csl'
---



```{r include=FALSE}
library(officedown)
```

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
require("knitr")
# opts_knit$set(root.dir = "..") #  I put this in to raise up the working directory from the reports folder (where the rmd lives) to one level above
```


```{r, launch_scripts, echo=FALSE, fig.show='hide', include=TRUE, warning=FALSE}
# This is the cleaning and pre-processing described above.
# source("./scripts/SBEP_staff_script_q3.R", print.eval = FALSE, echo = TRUE, keep.source = TRUE)
 # source("./scripts/0_SR_libraries_and_load.R", echo = FALSE)


 source("C:\\Users\\Alexander Fradera\\OneDrive - University of Glasgow\\DClin\\Deliverables\\Systematic Review\\Writeup\\DCLIN_SR_git\\scripts\\model_making.R", echo = FALSE)


# citation("metafor") etc

```
```{r captions, include=FALSE, eval=TRUE}
library(captioner)

#set the table caption styling
knitr::opts_chunk$set(tab.cap.pre = "Table", tab.cap.sep = " ")


figure_nums <- captioner(prefix = "Fig.")
table_nums <- captioner(prefix = "")
sup_figure_nums <- captioner(prefix = "Supplementary Fig.")
sup_table_nums <- captioner(prefix = "Supplementary Table")


# Supplementary figure definition (keep same as in Appendix for order)

sup_table_nums(name="diagnoses", caption = "Summary of diagnoses recorded for included studies") 

```



# Abstract

Chronic pain is increasingly prevalent with age and impairs cognitive function, but it is not known if it can affect performance on cognitive screening tests commonly used to detect dementia. This systematic review and meta-analysis (SR/MR) aimed to assess this. PRISMA guidelines were followed. Studies were included with participants age >18 with a pain-free control group and at least one chronic-pain group defined as self-reported pain lasting >3 months or a diagnosis included in lists curated by the  International Association for the Study of Pain. Embase (Ovid), Medline, PsycINFO (September 2021) and OpenGrey (January 2021) were searched. Risk of bias was assessed using the Joanna Briggs Critical Appraisal Checklist for Cross-Sectional Studies. Due to clustering of effects (multiple effects extracted from studies) a random effects multilevel modelling approach was used to calculate Hedges' g. `r length(unique(dfm2$unique_id))` effect size estimates were taken from `r length(unique(dfm2$study_id))` studies. The pooled SMD was `r format(round(main_model_output $pred, 2), nsmall = 2)` [95% confidence interval `r format(round(main_model_output $ci.lb, 2), nsmall = 2)` to`r format(round(main_model_output $ci.ub, 2), nsmall = 2)`]. Heterogeneity was high (I2 = `r model.i2$totalI2`%) and . Around half of studies were identified as being at a low risk of bias. There was no evidence of publication bias. Study bias factors limit interpretation of the findings as a whole, but sub-analyses suggest real sep
(and funding and registration)


# Introduction
## Rationale

Cognitive screening tools are measures designed to detect cognitive impairment through brief means, typically within 20 minutes [@Cullen2007]. These either target one highly predictive ability or core domains (language, memory, attention) using a minimal set of items. While cognitive screening is used to investigate a range of  phenomena such as brain tumors, psychiatric disorders, and traumatic brain injuries [@RoebuckSpencer2017], screening for dementia is commonly the rationale for their development. 


In some instances low cognitive screen scores that corroborate deficits reported at clinical interview may help clinicians reach a diagnosis. More commonly, screening scores aid in determining the need for more in-depth assessment, which is typically time-intensive and cognitively demanding. Cognitive screens should thus be sufficiently sensitive - correctly identifying when followup is warranted, to maximise early diagnosis - and specific - avoiding putting people onto an unnecessary investigative pathway [@Cullen2007].

Key to this diagnostic accuracy is understanding how other factors may influence screen performance. For instance, while the screening measure Addenbrooke's Cognitive Examination-III (ACE-III) appears reasonably robust to levels of premorbid intelligence of the test-taker [@Stott2017], other frequently used screens such as the Mini-Mental Scale Examination (MMSE) and Montreal Cognitive Assessment (MoCA) show an influence of intelligence [@Alves2013], leading those researchers to recommend premorbid IQ scores be considered alongside the results of these tests. @Dupuis2016 report a three-point decrement on the MoCA when completing the measure under noisy conditions.

These concerns are also  relevant when a population may have comorbid conditions that also affect cognition, such as chronic pain. When someone experiences chronic pain they are more likely to report problems with memory, attention or thinking [@McCracken2001]. Pain is known to affect performance on neuropsychological cognitive tests and batteries on domains including attention, speed of information processing and executive function, as described by @Moriarty2011. These authors note that potential mechanisms for this include depletion of resources, disrupted attention due to pain symptoms, and in the case of chronic pain possible longer-term changes to the nervous system due to sustained pain or the condition that produces it; concomittant analgesic may also impact cognitive performance. As chronic pain is more prevalent with aging [@Schofield2007], it is important to understand whether the experience impacts cognition sufficiently to result in alterations of cognitive screen performance.


Further reviews provide more detail on the impact of chronic pain on aspects of cognition. Meta-analyses of performance at working memory  are described in  @Berryman2013 and at executive function in @Berryman2014. For rheumatoid arthritis specifically @Pankowski2022 present meta-analyses showing cognitive impairment across several domains, and  @Meade2018 note impairments particularly in memory, attention and verbal function. A review of fibromyalgia by @SchmidtWilcke2010 summarises problems in free recall, working memory and a mixed pattern of results around attention. 

In the main these studies do not focus on cognitive screening tools. The primary exception is @Pankowski2022 which reports estimated effect sizes for two such measures, the Mini-Mental State Inventory (based on eight comparisons) and the Montreal Cognitive Assessment (based on three comparisons), finding respective standardised mean differences of .66 [95% CI 0.42- 0.9] and 1.27 [95% CI .68-1.87]. This is suggestive that pain conditions may be associated with poorer cognitive screen performance. However, this may not generalise to other conditions, especially as other mechanisms for cognitive impairment are suspected for rheumatoid arthritis [such as impact on intracranial circulation, see e.g. @Olah2017].


## Objectives

The aim of this study was to conduct a systematic review/meta-analysis to assess the impact of living with chronic pain upon cognitive screen performance.


# Method

The study followed the PRISMA guidelines for reporting SRs and MAs [@Moher2009].  The protocol of the study was registered with the International Prospective Register of Systematic Reviews (PROSPERO) [registration number: CRD 42021272835] and published elsewhere (https://osf.io/jsqxn/).

## PRISMA: Eligibility criteria

The review focused on primary research that satisfied a set of PECO criteria - (P)opulation, (E)xposure, (C)omparator, (O)utcomes - defined as follows: in participants of any sex aged 18 or over (P) investigate the effect of living with chronic pain (E) versus not living with chronic pain (C) on cognitive screening tool performance (O).

Study design was not strictly defined and could include cross-sectional designs as well as experimental designs including those introducing treatment, unless the available screening data was confounded by the treatment. Presence of a control group who were not experiencing chronic pain was a key eligibility criteria. 

Definitions of cognitive screening tools are varied and the subject of a number of previous systematic reviews [eg @Ashford2008, @Cullen2007] The decision was made to utilise a practitioner definition provided by the @AlzheimersSociety2013 which reports nine screens agreed by UK clinicians to be appropriate for common clinical practice. These were: Addenbrooke's Cognitive Examination-III (ACE-III), Abbreviated Mental Test (AMT), Mini-Cog, Montreal Cognitive Assessment (MoCA), Mini Mental State Examination (MMSE), 6-item Cognitive Impairment Test (6CIT), Hopkins Verbal Learning Test (HVLT), Test for the Early Detection of Dementia (TE4D-Cog), and Test Your Memory test (TYM). Studies using different editions and language variants of these screens were also eligible. 


## PRISMA: Information sources

PROSPERO and Epistimonikos were searched for similar ongoing or recently completed SRs on 5th April 2021. 
Searches of bibliographic databases were conducted on 17th September 2021 via the following platforms: Ovid for Embase (1947-present), and EBSCOhost for Medline (1946-present), and PsycINFO; a mapping of articles identified in preliminary searches showed saturation by the use of these databases.  OpenGrey (System for Information on Grey Literature in Europe) was separately searched for identification of relevant non peer-reviewed research in January 2021.





## Search strategy

Database search was performed using the PECO model described above. It used medical headings and keywords associated with pain and cognitive screening instruments. Guidance on term inclusion drew on the PubMed PubReMiner package and use of the R package litsearchr to use initial keywords (eg for exposure, "pain OR fibromyalgia OR migraine OR neuropathic") to generate networks of related keywords, together with a review of  a publication by the International Association for the Study of Pain (IASP) providing lists of chronic pain conditions [@Merskey1994, lists 1A, 1F, 1H]. 

Population was defined by use of Medical Headers. Exposure was ultimately operationalised in title and abstract by identification of key pain-related conditions (fibromyalgia, arthritic and rheumatic conditions), chronic adj/5 pain or headache/migraine, or report of a standardised pain measure (e.g. McGill Pain Questionnaire); where available medical headers for pain were used. Outcome was operationalised by full names and abbreviations of the nine cognitive measures in title and abstract and where available tests and measures fields. No comparator information was used to define the search parameters. Searches were initially piloted in PsychInfo before adaptation for use in the other databases. Full search strategies for each database can be found in Appendix XXXX. 

Hand-searches were made prior to search execution to identify relevant studies that met criteria, using the initial keywords described above and reviews identified by searching Epistimonikos and PROSPERO. Further studies were identified through searching of a review article discoverd subsequent to completing the search. Due to the number of studies obtained through the search process the decision was made not to conduct back- or forward-citation searches.

## Selection process

After acquiring search results and removal of duplicates, two initial co-review stages were taken by reviewers 1 (AF) and 2 (JM): firstly to calibrate the eligibility checklist on 10 title-abstracts and agree refinements, then to independently screen 120 titles and abstracts against inclusion criteria, and finally to discuss discrepancies in judgment until reaching consensus on all cases, consulting with a third author where necessary. AF then completed sifting of titles and abstracts. Following this step the full texts of retained studies were reviewed starting with an eligibility calibration on 2 papers, followed by independent screening of 20 further full texts by the same two reviewers, addressing discrepancies in a similar manner. AF completed the full text review on all remaining results. Authors were contacted when full articles are unavailable (n=1). Studies had to meet one of two criteria for chronic pain: experience of pain at one or more locations for at least three months at the time of study involvement, or diagnosis with a condition known to involve chronic pain such as fibromyalgia, arthritis, or any condition found on the list of IASP diagnoses. Full guidelines for reviewers for both title-abstract and full-text screening can be found in APPENDIX XX.

## Data extraction and items

Data was extracted by AF, with JM performing a check to ensure accurate extraction on 5 consecutive papers, which was achieved after 8 papers. Authors were contacted when data was partially incomplete (e.g. means but no standard deviations), potentially duplicated data within another study, or appeared to contain errors. 

Relevant data included type of pain condition, participant details,  measures of pain and mood and scores on the cognitive screening test, as well as whether the test was key or incidental to the study (e.g. a baseline measure). The cognitive screen data extracted was in the form of reported means and standard deviations, or medians and inter-quartile range where this was available. A data dictionary showing the full variables of interest can be found in APPENDIX XXX.

In some instances, studies reported data on a cognitive screen only after using the screen performance to exclude participants. A decision was made to include this data while noting the presence of this as impacting study quality. Where data was provide on multiple outcomes (cognitive screens) all outcomes were extracted.

## Quality of evidence / Quality assessment tool

Study quality was assessed using the JBI Critical Appraisal Checklist for Analytical Cross Sectional Studies (REF). This scale rates studies on criteria such as inclusion criteria, description of study, outcome measurement and strategies to deal with confounding factors. The checklist includes items that required some adaptation for relevance in this review, which was supplied to both reviewers as supplementary guidance that can be found in appendix XXXY. Of the eight criteria, one (item 4) was judged to be redundant in reviewing the studies extracted, and was not included in the quality assessment. No overall risk of bias score is produced using this tool. Reviewers AF and JM independently reviewed quality for 5 studies  before meeting to resolve discrepancies and use this to amend the supplementary guidance for further quality assessment, which was completed by AF.

## Statistical analyses  - effect measures and synthesis

Cognitive screen means and standard deviations were used to compute standardised mean differences in the form of Hedge's g using the approach described by @Hedges1985. Where data was presented in the form of median and inter-quartile range, this was converted into estimated mean and standard deviation using the estmeansd r package using the Boxâ€“Cox method described in @McGrath2020. Scores reflect the size of effect for membership of the chronic pain group versus control with larger scores reflecting greater impairment in the chronic pain group.

Analysis was conducted using the metafor package [@Viechtbauer2010] with a random-effects model using restricted maximum-likelihood ("REML") estimation to measure between-study variance and production of a Wald-type confidence interval. Individual effect sizes and aggregated effect size were depicted using forest plots. A multivariate meta-analytic approach was taken as a number of studies reported more than one comparison of cognitive screen scores that fit  review criteria due either to multiple chronic pain groups compared to one control group or two cognitive screens administered across participants.  In most instances this led to generating multiple standard mean differences per study, leading to interdependency between outcomes. In one case, it was judged that groups could be more appropriately merged to produce a single standardised mean difference. 

Due to these interdependencies we used a multivariate approach to pooling data  which involved computing a covariance matrix estimating these interdependencies, including information about the relationships between cognitive screen scores, that was incorporated into the model. The matrix is used to form weightings for estimates and creates a downweighting of estimates from studies with multiple comparisons. We also explored whether results differed when employing robust variance estimation methods to further account for non-independence. Fuller details can be found in appendix XXXXX. 

Two types of sub-group analysis were considered: one decomposing out data relating to a single cognitive screening measure, and another decomposing data relating to a single pain condition. These analyses were attempted for situations where five or more comparisons were available. 

## Reporting bias assessment

To identify whether results may be missing in a non-random fashion due to reporting bias, a funnel plot was produced. In many studies our measure of interest (cognitive screen) was incidental to the wider motive for the research (eg was used merely to report sample characteristics). In these situations we felt it was not meaningful to investigate missingness of effects, and decided to limit the funnel plot to studies where the findings hinged on the cognitive screening data, in order to identify whether there has been a systematic under-reporting of non-significant findings.

## CERTAINTY ASSESSMENT?

# Results

## Study selection

A total of 3505 records were identified, 485 of which were initially identified as duplicates. Following two sifting stages (an additional step was made when accessing full-texts to dispose of a large number of conference abstracts), 140 were examined in full text. This led to 44 studies that met the inclusion criteria together with a further 7 from hand-search. For MA calculations, 62 comparisons were available from the 51 studies.

![Fig X Overview of selection process](C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/graphs/flowchart.jpg)

7 articles were identified by hand-searching prior to and following the search. We excluded one study [@Han2013] that appeared to meet the inclusion criteria by explicitly referencing chronic pain but used a different criteria of pain lasting at least one month (rather than three), with no diagnostic information or mean duration reported to verify that this would constitute chronic pain by our criteria. 

## Study characteristics



``` {r, info, echo = F} 
#### variable creation for below text

su_scr <- dfm_mod %>%
  group_by(cognitive_name)%>%
  count()

numbs <- ((su_scr[1:5,2])) # still a weird object so use [[]] to subset

names <- levels(dfm_mod$cognitive_name)

```

The search process led to the extraction of `r length(unique(dfm2$unique_id))` effect size estimates from `r length(unique(dfm2$study_id))` studies. Data from `r sum(dfm2$n_treat)` people experiencing chronic pain and `r sum(dfm2$n_cont) ` pain-free controls was extracted.  There were `r numbs[[1,1]]` comparisons involving the `r names[1]`, `r numbs[[2,1]] ` for the `r names[2]`, `r numbs[[3,1]]` for the `r names[3]`, `r numbs[[4,1]]` for the `r names[4]`, and `r numbs[[5,1]]` for the `r names[5]`. In `r sum(dfm_mod$cognitive_focus=="key")` comparisons, the screen was critical to the study focus and `r sum(dfm_mod$cognitive_focus=="incidental")` where the screen was used merely to inform the description of samples.





``` {r, overviewtab, echo = F}
### Study characteristics table - keep here


library("flextable")
library("ftExtra")
library("dplyr")
library("officer")
library("openxlsx")
library("ggpubr")

set_flextable_defaults(
  font.size = 10, theme_fun = theme_vanilla,
  padding = 6,
  background.color = "#EFEFEF")


overview <- read.xlsx("C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/data/study_overview_sheet.xlsx")
overview <- overview %>%
  mutate(biblio_comp = paste0("@",biblio))

pres_overview <- select(overview, study_id, biblio_comp, country)

pres_dat <- dfm_e %>%
  select(study_id, sample_treat, n_treat, age_mean_treat, age_sd_treat,  n_cont, matching_education, cognitive_name,cognitive_variant )

test <- left_join(pres_dat,pres_overview, by="study_id")
test <- test %>%
  arrange(biblio_comp) %>%
  relocate(biblio_comp, country) %>%
  select(!study_id)

test <- test %>%
  mutate(age_mean_treat = round(age_mean_treat, 2),
         age_sd_treat = round(age_sd_treat, 2),
         age_sum_treat = paste0(age_mean_treat," (",age_sd_treat,")"),
         age_sum_treat = if_else(is.na(age_sum_treat),"NR", age_sum_treat),
         cognitive_name = ifelse(cognitive_name %in% c("ACE"),
         						paste0(cognitive_name,"-", cognitive_variant),paste0(cognitive_name))
         )


tab1 <- test %>%
  select(!c(age_mean_treat,age_sd_treat,cognitive_variant)) %>%
  arrange(biblio_comp) %>%
  relocate(biblio_comp) %>%
  flextable() %>%
  colformat_md() %>%
  autofit()
tab1 <- set_table_properties(tab1, layout = "autofit")
tab1 <- set_header_labels(tab1, 
                  biblio_comp = "Authors", 
                  country = "Country",
                  sample_treat = "Pain group",
                  matching_education = "Education",
                  n_treat = "Patient group size",
                  n_cont = "Control group size",
                  cognitive_name = "Screen",
                  age_sum_treat = "Patient age - Mean (SD)") %>%
	add_footer_lines("NR = not reported. MSK = Musculo-skeletal condition. SLE = Systemic Lupus Erythematosus. Test abbreviations: ACE = Addenbrooke's Cognitive Examination (R = Revised, III = 3rd edition). HVLT = Hopkins Verbal Learning Test. MMSE = Mini-Mental State Examination. MoCA = Montreal Cognitive Assessment. TYM = Test Your Memory test.")

tab1

# put this outside the block to work <!---BLOCK_LANDSCAPE_START---> <!---BLOCK_LANDSCAPE_STOP--->
# as per https://ardata-fr.github.io/officeverse/officedown-for-word.html#insert-sections
```


**change NA to NR for Li et al age**
**add footnote with abbreviations**


## Risk of bias  in studies

```{r qualityfails}
inclusiondesc <- quality_scores_vals %>% count(J1)
sampledescribe <- quality_scores_vals %>% count(J1)
painrep <- quality_scores_vals %>% count(J3)
confoundrep <- quality_scores_vals %>% count(J5)
confoundcont <- quality_scores_vals %>% count(J6)
cogreport <- quality_scores_vals %>% count(J7)
cogcutoff <- quality_scores_vals %>% count(J8)
toggles <- quality_scores_vals %>% count(toggle)
```

Only three comparisons (from two studies) met every JBI criteria. The majority of comparisons (`r inclusiondesc[3,2]`) did adequately describe sample details and inclusion criteria as per items 1 and 2. However the majority of comparisons (`r painrep[1,2]`) failed to meet criteria 3 for providing pain measurement in both control and pain groups, and failed to meet criteria 5 (`r confoundrep[1,2]`) due to non-reporting of mood, education, age or medication information. Half of comparisons (`r confoundcont[1,2]`)  failed to meet criteria 6, for ensuring groups were controlled for education and age and around half  (`r cogreport[1,2]`)  failed to provide information about administration of the cognitive screen as per criteria 7. Our operationalisation of criteria 8 concerned the use of screening scores to filter out low-scoring participants, thus implementing a floor on scores^[We excepted two cases where the threshold was very low, 10 or 14/35 on the MMSE, which contrasted with other cutoffs of eg 24, 28; the rationale for this is that in the latter case, the cutoff crossed-over with score ranges seen within the reported samples (within a 68% coverage interval), whereas the former was set to exclude the severely cognitive impaired as part of exclusion criteria]; `r cogcutoff[1,2]` studies showed bias on this criteria.

Comparisons with low risk of bias were those meeting these criteria: 

- adequate exclusion criteria (J1)
- controlled or matched for age and education (J6)
- did not employ a cut-off that prevented detection of poor performance (J-8)

This resulted in  `r toggles[2,2]` comparisons with a low risk of bias.

![Fig X. Risk of bias plot](C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/graphs/quality_simp.png)


```{r moodsum}

mooddiffs <- aim2 %>%
  group_by(q6_mood_confound) %>%
 count()

mooddiffs_x <- mooddiffs[1:4,2]

```


Note that the evaluation of confounders reported in item 6 does not include mood, which is a considered prevalent and co-occurring symptom with pain, forming part of a symptom cluster [@Davis2016]. Mood was significantly higher for the pain group in `r mooddiffs_x[[2,1]]` of the studies, with `r mooddiffs_x[[3,1]] +  mooddiffs_x[[4,1]]` not reporting mood and only `r mooddiffs_x[[1,1]]` reporting similar levels of mood. 



## Results of syntheses


For the meta-analytic calculations we first employed the random effects multi-level meta-analytic model on all available comparisons. The pooled SMD estimate (with a positive effect denoting degree of impairment in pain groups) was was `r format(round(main_model_output $pred, 2), nsmall = 2)` with the 95% confidence interval ranging from `r format(round(main_model_output $ci.lb, 2), nsmall = 2)` to `r format(round(main_model_output $ci.ub, 2), nsmall = 2)`. This describes the range within which we expect the average effect size to fall (based on our dataset). 

A comparison of these heterogeneity with the within-comparison variances suggests that these studies are capturing different effects (Cochran's Q = `r round(che.model$QE,2)`, p < 0.0001). The estimated variance components were 	$\tau_{Level3}^2$ = `r round(che.model$sigma2[1],3)` and $\tau_{Level2}^2$ = `r round(che.model$sigma2[2],3)`, meaning that between-study variation accounts for `r model.i2$results[3,2]`% of the total variation, whereas `r model.i2$results[2,2]`% was due to variability between multiple comparisons within a single study. 

![Fig X. Forest plot](C:/Users/Alexander Fradera/OneDrive - University of Glasgow/DClin/Deliverables/Systematic Review/Writeup/DCLIN_SR_git/graphs/Forest_rich.png)

#### Subgroup analysis

We re-analysed the dataset using robust variance estimators which produced almost identical outputs to the standard multi-level method, . A simple random effects model produced a higher pooled SMD estimate, suggesting that effect non-independence was bearing on findings and justifying the multi-level approach. In addition we completed a sensitivity analysis on those comparisons with a low risk of bias, and from these comparisons sub-analyses with i) MMSE screen only ii) MoCA screen only iii) arthritis pain condition only, employing the multi-level approach when the dataset contained clusters. We also conducted sub-analyses for the fibromyalgia pain condition, but note that this contains high- and low-risk of bias comparisons as too few comparisons were otherwise available. We also analysed a subgroup containing the `r mooddiffs_x[[1,1]]`  comparisons where groups had similar levels of mood (all also rated as low risk).Study heterogeneity remained fairly high across these analyses except for with the smallest subgroup (with mood similar across groups), but  $I^2$ should be interpreted carefully when study numbers are small [@Hippel2015]. These are reported in Table XX.

```{r, regressiontab}

tab2 <- total_table %>%
    select(-order,-justes) %>%
    flextable() %>%
  colformat_md() %>%
  autofit()
tab2 <- set_table_properties(tab2, layout = "autofit")
tab2 <- set_header_labels(tab2, 
                  type = "Dataset", 
                  method = "Meta-analytic method",
                  conf = "SMD [95% confidence interval]",
                  pval = "Significance level",
                  i2 = "I2",
                  i2_2 = "Within-cluster (study) I2",
                  i2_3 = "Between-cluster (study) I2") %>%
	add_footer_lines("Footnotes")

tab2

```


## Reporting biases

Fig XXXX shows a funnel plot including studies where the cognitive screen was key to the study purpose. This uses the trim-and-fill method [@Duval2000] to interpolate study effect sizes that would be expected from the extracted results, suggesting where low-powered and non-significant effects may be absent from the distribution.

``` {r, funnelling, echo = F}
 dfm_key<- dfm_mod %>%
    filter(cognitive_focus == "key")

res_key <- rma(yi, vi, data=dfm_key)
res_tri <- trimfill(res_key)
funnel(res_tri, legend= TRUE, main = "Funnel Plot (focused studies)")

# what about without outlier?
#dfm_key_out <- filter(dfm_mod, yi <3)

#res_key_out <- rma(yi, vi, data=dfm_key_out)
#res_tri_out <- trimfill(res_key_out)
#funnel(res_tri_out, legend= TRUE, main = "Funnel Plot (focused studies) - no outlier") 

```

The plot suggests that no additional low-powered, non-significant effects would be expected given the observed distribution, providing no evidence of reporting biases. 



# Discussion

## Summary of findings

The pooled size of effects from studies with a risk of low bias was XXX, suggesting that chronic pain does impact.
However, the high levels of heterogeneity suggest that the sources of this effect may be manifold. Sub-analyses on the two most represented screens (MMSE and MoCA) saw some reduction of heterogeneity, and saw larger effect sizes for the MoCA, suggesting that the screens may be differentially affected by chronic pain. This may be because the MMSE does not draw on executive function in its assessment [@NieuwenhuisMark2010], which is a domain that chronic pain does appear to affect. 

Analysis based upon subgroups did xxxx...

Inflammatory diseases are increasingly understood to have neurological implications, meaning that patients whose chronic pain stems from these diseases may score differently on chronic screens because of pain and the direct action of the disease such as premature immunosenescence [@Petersen2015]. There is evidence, for instance, that patients with rheumatoid arthritis attain poorer MoCA scores than controls with similar levels of bodily pain [@Kim2018].

There are also brain changes associated with chronic headache, for instance medication-overuse headache with increased white matter hyperintensity [@Xiang2021] and changes in functional connectivity in the neostriatum @Chen2016; however note that these neurological features may not be tied to cognitive function.

### Comparison with previous research

Within people living with arthritis, @Pankowski2022 found effect sizes that roughly resemble the ones within the current analysis. We note that in both cases estimates were based on small numbers of studies (studies/comparisons: Pankwosis MMSE = 7/8, MoCA 3/3, current study MMSE 5/7, MoCA 4/4) and we excluded two MMSE comparisons and one MoCA comparison included in their analyses due to risk of bias.

Previous research has reported patients with fibromyalgia to have higher prevalence of cognitive deficit based on screen performance compared to other forms of pain [e.g. neuropathic or mixed pain, @RodriguezAndreu2009] although score ranges do not differ drastically. In our study....

In our dataset the effects reported from studies on musculoskeletal pain were somewhat lower than the main reported effect. This may tally with the findings above, or relate to severity of pain, which was not analysed in this study. @Chen2011 reported that patients with self-reported musculoskeletal chronic pain who scored in the upper quartile on the Brief Pain Inventory Severity Scale were twice as likely as those in the lower quartile to obtain an MMSE score below 24 (18.5% versus 9.7%). However, @Bosley2004 found no difference in MMSE scores between groups with significantly different pain intensity scores (MPQ-SF).

## Limitations of evidence

Our quality assessment found the majority of studies failed to meet all JBI criteria. Some of these issues, while introducing risk of bias for our purposes may be necessary for invidual studies, for instance the use of screening tools to screen out lower scoring participants. The infrequency of conducting pain  measurements on both controls and patient groups is also understandable, but does prevent an objective comparison of pain levels that could add more confidence to findings. Most studies are uncontrolled for mood and medication differences between pain and control groups, which may be unavoidable with typically presenting samples. @Rock2014 report small to moderate effects of depression on cognition, although we should note that we do not have measures of sample caseness for depression (and many did exclude participants on the basis of a depression diagnosis). Commonly prescribed medication for rheumatoid arthritis have been associated with cognitive impairment [on methotrexate, see @Pamuk2013/;/ on glucocorticoid therapy, see @Coluccia2008]. However a study by @Gogol2014a looked at the impact of opioidal medication on the MMSE and reported no effect. Given that cognitive performance is known to be influenced by age and education, failure to match for these measures is a clear opportunity for bias to be introduced (although some studies may still mitigate this by controlling within a subsequent analysis of interest). 

If pain were under-reported by people with cognitive impairment this would introduce systematic error into the findings. There is evidence of such under-reporting for people with a dementia diagnosis [reviewed by @Scherder2005]; however the current review excluded studies on that diagnosis or with similar neuropsychological impairments. Other research shows that less pronounced cognitive deficits are not associated with changes in pain perception or reporting [see e.g. @Kunz2009 and @Docking2014].
 
## Limitations of review processes

The review includes individuals defined as experiencing chronic pain due to self-rating or diagnosis with one of several pain conditions. As discussed, various conditions may have cognitive consequences outside of their effect via eliciting pain. We felt it important to include forms of chronic pain that could arise for clinicians in the assessment of dementia, such as arthritis or fibromyalgia, however we accept that this contributes to the heterogeneity of the findings. 

In addition, this review chose to limit its cognitive screens to a fairly narrow list, in contrast with the number available^[In preparation for this review, one author(AF) collated a list of over 100 measures described across systematic reviews of cognitive screens]; the decision to rely on a pre-existing criteria was again to align with clinical usage. We recognise that this list was made by UK clinicians and that this may limit generalisability. We also note that besides the MoCA and MMSE, there were very few studies extracted for the other cognitive screens, meaning that this review heavily focuses on just two measures. However, we note that the studies included come from a range of countries including multiple from the Middle East, South America and Asia, avoiding a focus merely on European and North American samples.

This review focused on studies comparing pain and pain-free groups, meaning that we were not able to include longitudinal studies examining change over time or due to interventions, which could give insights into how cognitive screen performance follows the course of pain condition/fluctuation in pain experience. 


## Meaning and implications

This systematic review and meta-analysis offers point effect size estimates as high as `r max(total_table$justes)` and as low as `r min(total_table$justes)` with the estimate for low risk of bias studies at `r total_table$conf[3]`. No 95% coverage interval overlapped with 0, suggesting that across diverse groups experiencing chronic pain, cognitive screen performance is lower than for control groups, even when low mood, which frequently co-occurs with pain, is not in the picture. Clinicians may want to consider this when administering cognitive screens, and whether the presence of pain may bear on screen performance  - potentially moreso for the MoCA tool which taps executive function, something that is impacted by pain. Crucial to consider however is whether for at least some conditions involving pain, cognitive impairment may be a marker for later severe decline. Future work could look at screen performance by components, to identify if there are certain areas where pain impacts and others where it does not affect results.

[*NB I would like to report back actual effect sizes in a practical way, eg MMSE real score differences. But not sure whether I should be taking the average standardised effect size discovered and translating that back  into a score, or doing a calc with eg MMSE studies to find the (unstandardised) mean difference there. Thoughts?*]



# Other information
(To do)

24c Describe and explain any amendments to information provided at registration or in the protocol.
Support 25 Describe sources of financial or non-financial support for the review, and the role of the funders or sponsors in the review.
Competing interests 26 Declare any competing interests of review authors.
Availability of data, code, and other materials
27 Report which of the following are publicly available and where they can be found: template data collection

# References