---
title             : "Does the presence of chronic pain affect scores on cognitive screening tests/brief cognitive measures? A Systematic Review"
shorttitle        : "Title"

author: 
  - name          : "Alex Fradera"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "School of Health & Wellbeing, College of Medical, Veterinary and Life Sciences, University of Glasgow, Clarice Pears Building, 90 Byres Road, Glasgow, G12 8TB"
    email         : "alexander.fradera@glasgow.ac.uk"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Data curation"
      - "Formal analysis"
      - "Investigation"
      - "Methodology"
      - "Project administration"
      - "Software"
      - "Visualisation"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Jessica McLaren"
    affiliation   : "2"
    role:
      - "Investigation"
      - "Methodology"
      - "Writing - Review & Editing"
      
  - name          : "Lisa Gadon"
    affiliation   : "3"
    role:
      - "Conceptualization"
      - "Methodology"
      - "Writing - Review & Editing"
      
  - name          : "Breda Cullen"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Methodology"
      - "Writing - Review & Editing"

      
  - name          : "Jonathan Evans"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Methodology"
      - "Writing - Review & Editing"

affiliation:
  - id            : "1"
    institution   : "University of Glasgow"
  - id            : "2"
    institution   : "NHS Ayrshire and Arran"
  - id            : "3"
    institution   : "NHS Greater Glasgow and Clyde"

authornote: | 


abstract:   "(ref:abstract)"


  
keywords          : "dementia, pain, cognition, cognitive screen, meta-analysis"
wordcount         : "X"

bibliography      : '../SR_biblio.bib'

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no  #yes

figurelist        : no
tablelist         : no
footnotelist      : no

# classoption       : "man"
classoption       : "man, donotrepeattitle"
header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\singlespacing}
  - \AtBeginEnvironment{lltable}{\singlespacing}
  - \AtBeginEnvironment{tablenotes}{\doublespacing}
  - \captionsetup[table]{font={stretch=1.5}}
  - \captionsetup[figure]{font={stretch=1.5}}

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
# output            : papaja::apa6_pdf
output            : papaja::apa6_word
---


\newpage

```{r setup, include = FALSE}
library("papaja")
library(officedown)
library(here)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
require("knitr")
# opts_knit$set(root.dir = "..") #  I put this in to raise up the working directory from the reports folder (where the rmd lives) to one level above
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```



```{r, launch_scripts, echo=FALSE, fig.show='hide', include=TRUE, warning=FALSE}
# This is the cleaning and pre-processing described above.

source(here('scripts', 'model_making.R')) 


```


```{r captions, include=FALSE, eval=TRUE}
library(captioner)

#set the table caption styling
knitr::opts_chunk$set(tab.cap.pre = "Table", tab.cap.sep = " ")


figure_nums <- captioner(prefix = "Fig.")
table_nums <- captioner(prefix = "")
sup_figure_nums <- captioner(prefix = "Supplementary Fig.")
sup_table_nums <- captioner(prefix = "Supplementary Table")


# Supplementary figure definition (keep same as in Appendix for order)

sup_table_nums(name="diagnoses", caption = "Summary of diagnoses recorded for included studies") 

```

(ref:abstract) Cognitive screening tests can identify potential dementia by indicating a concerning level of cognitive impairment. The older populations for whom this is most relevant are more likely to experience chronic pain, which also impairs cognitive function, but pain's impact on cognitive screening tests specifically remains unknown. We conducted a systematic review and meta-analysis (SR/MA) following PRISMA guidelines evaluating cognitive screening scores in studies involving participants with chronic pain compared with a pain-free control group. Our question was whether the presence of chronic pain (self-reported or based on diagnosis) was associated with poorer performance on these screens, and to identify the heterogeneity across groups and screens. The `r length(unique(dfm_mod$study_id))` studies identified yielded `r length(unique(dfm_mod$unique_id))` effect size estimates. The pooled g was `r format(round(main_model_output $pred, 2), nsmall = 2)` [95% confidence interval `r format(round(main_model_output $ci.lb, 2), nsmall = 2)` to `r format(round(main_model_output $ci.ub, 2), nsmall = 2)`]. Heterogeneity was high for the full model ($I^2$ = `r round(model.i2$totalI2,2)`%)  with some reductions in sub-analyses. Around half of studies were identified as being at a low risk of bias. There was no evidence of publication bias. As a whole, this analysis suggests medium to large effect sizes on cognitive screen performance when people are living with chronic pain. We suggest that clinicians should consider the effect of chronic pain when cognitive screens are employed, and the need to clarify the effect pain has on different screen components to aid their effective use with these populations.

# Introduction
## Rationale

Cognitive screening tools are measures designed to detect cognitive impairment through brief means, typically within 20 minutes [@Cullen2007]. These target either one highly predictive ability or core domains (e.g. language, memory, attention) using a minimal set of items. While cognitive screening is used for many conditions such as brain tumours, psychiatric disorders, and traumatic brain injuries [@RoebuckSpencer2017], a common rationale for their development is to screen for dementia. 


In some instances low cognitive screen scores that corroborate deficits reported at clinical interview may help clinicians reach a diagnosis. Screening scores also aid in determining the need for more in-depth assessment conducted by clinical psychologists and neuropsychologists, assessment which is typically time-intensive and cognitively demanding. Cognitive screens should thus be sufficiently sensitive - correctly identifying when follow-up is warranted, to maximise early diagnosis - and specific - avoiding putting people onto an unnecessary investigative pathway [@Cullen2007].

Key to diagnostic accuracy is understanding how other factors may influence screen performance. For instance, while the screening measure Addenbrooke's Cognitive Examination-III (ACE-III) appears reasonably robust to levels of premorbid intelligence of the test-taker [@Stott2017], other frequently used screens such as the Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA) show an influence of intelligence [@Alves2013], leading those researchers to recommend premorbid IQ scores be considered alongside the test results. Environmental factors have also been noted as influential, with @Dupuis2016 reporting a three-point decrement on the MoCA when completing the measure under noisy conditions. Performance concerns also apply to populations with co-morbid conditions that may affect cognition, such as chronic pain. 

When someone experiences chronic pain they are more likely to report problems with memory, attention and thinking [@McCracken2001]. Pain is known to affect performance on neuropsychological tests and batteries, on domains including attention, speed of information processing and executive function, as described by @Moriarty2011. 

These authors note that potential mechanisms driving this impairment include resource depletion and disrupted attention due to pain symptoms; for chronic pain, further possibilities are concomitant analgesic use and longer-term neurological changes due to the pain condition or sustained experience of pain. As chronic pain is more prevalent with aging [@Schofield2007], clinical psychologists and neuropsychologists who work in dementia services are likely to be presented with cognitive screening scores for people living with chronic pain, and to make judgments on how to interpret these.  It is therefore important to understand whether the experience impacts cognition sufficiently to result in alterations of cognitive screen performance.

Further reviews provide more detail on the impact of chronic pain on aspects of cognition. Meta-analyses of performance in working memory  are described by  @Berryman2013 and in executive function by @Berryman2014. For rheumatoid arthritis specifically, @Pankowski2022 present meta-analyses showing cognitive impairment across several domains, and  @Meade2018 note impairments particularly in memory, attention and verbal function. A review of fibromyalgia by @SchmidtWilcke2010 summarises problems in free recall, working memory and a mixed pattern of results around attention. 

In the main these studies do not focus on cognitive screening tools. The primary exception is a review on rheumatoid arthritis by @Pankowski2022 which reports estimated effect sizes for two such measures, the MMSE (based on eight comparisons) and the MoCA (based on three comparisons), finding respective standardised mean differences of .66 [95% CI 0.42- 0.90] and 1.27 [95% CI 0.68-1.87]. This suggests pain conditions may be associated with poorer cognitive screen performance. However, this may not generalise to other conditions, especially as other mechanisms for cognitive impairment are suspected for rheumatoid arthritis [such as impact on intracranial circulation, see e.g. @Olah2017].


## Objectives

The aim of this study was to conduct a systematic review/meta-analysis to assess the impact of chronic pain upon cognitive screen performance.


# Materials and Methods

The study followed the PRISMA guidelines for reporting SRs and MAs [@Moher2009].  The protocol of the study was registered with the International Prospective Register of Systematic Reviews (PROSPERO) [registration number: CRD 42021272835] and published elsewhere (https://osf.io/jsqxn/); this includes an analysis plan released prior to the work being conducted. The protocol was updated 14th May 2022 to clarify that comparisons must involve pain-free controls.

## Eligibility criteria

The review focused on primary research that satisfied a set of PECO criteria - (P)opulation, (E)xposure, (C)omparator, (O)utcomes - defined as follows: in (P) participants of any sex aged 18 or over investigate (E) the effect of having chronic pain versus (C) controls without chronic pain  on (O) cognitive screening tool performance. Studies could include cross-sectional as well as experimental designs unless the available screening data was confounded by an introduced treatment. Studies were excluded if they involved samples with a diagnosed cognitive impairment due to a disease originating in the brain, such as stroke, traumatic brain injury or dementia.

Definitions of cognitive screening tools are varied and are the subject of a number of previous systematic reviews [e.g. @Ashford2008; @Cullen2007]. This SR utilised a practitioner definition provided by the @AlzheimersSociety2013 which reports nine screens agreed by UK clinicians to be appropriate for common clinical practice, being: Addenbrooke's Cognitive Examination-III (ACE-III), Abbreviated Mental Test (AMT), Mini-Cog, Montreal Cognitive Assessment (MoCA), Mini Mental State Examination (MMSE), 6-item Cognitive Impairment Test (6CIT), Hopkins Verbal Learning Test (HVLT), Test for the Early Detection of Dementia (TE4D-Cog), and Test Your Memory test (TYM). Studies using different editions and language variants of these screens were eligible. 


## Information sources

PROSPERO and Epistimonikos were searched for similar ongoing or recently completed SRs on 5th April 2021. 
Searches of bibliographic databases were conducted on 17th September 2021 via: Ovid for Embase (1947-present), and EBSCOhost for Medline (1946-present) and PsycINFO; a mapping of articles from preliminary searches showed these databases achieved saturation.  OpenGrey (System for Information on Grey Literature in Europe) was separately searched for identification of relevant non peer-reviewed research in January 2021.


## Search strategy

Database searches were designed using the PECO model described above.  Exposure was operationalised in title and abstract by identification of key pain-related conditions (fibromyalgia, arthritic and rheumatic conditions), chronic adj/5 pain or headache/migraine, or report of a standardised pain measure (e.g. McGill Pain Questionnaire); where available medical headers for pain were used. Population was defined by use of Medical Headers. Outcome was operationalised by full names and abbreviations of the nine cognitive measures in title and abstract and where available tests and measures fields. No comparator information was used to define the search parameters. Searches were initially piloted in PsychINFO before adaptation for use in the other databases. Full search strategies for each database can be found in Appendix 1. 

Hand-searches were made prior to search  to identify relevant studies that met criteria, using keywords and reviews identified by searching Epistimonikos and PROSPERO. Further studies were identified through a review article discovered subsequent to search completion. Due to the number of final studies obtained back- or forward-citation searches were deemed unnecessary.

## Selection process

After acquiring search results and removal of duplicates, two initial co-review stages were completed by reviewers 1 (AF) and 2 (JM). Stage one began by calibrating the eligibility checklist on 10 title-abstracts and agreeing refinements. After this both reviewers independently screened 120 titles and abstracts against inclusion criteria, discussing discrepancies in judgment until reaching consensus on all cases (consulting with a third author where necessary). AF then completed sifting of titles and abstracts.  In stage two the full-texts of two retained studies were reviewed by both reviewers to calibrate eligibility. These reviewers then independently screened 20 further full texts, addressing discrepancies as per stage one. AF completed the full text review on all remaining results. Authors were contacted when full articles were unavailable (n=1). Studies had to meet one of two criteria for chronic pain: experience of pain at one or more body locations for at least three months at the time of study involvement, or diagnosis with a condition known to involve chronic pain such as fibromyalgia, arthritis, or any condition found on the lists of chronic pain conditions provided by the International Association for the Study of Pain (IASP)  [@Merskey1994, lists 1A, 1F, 1H]. Full guidelines for reviewers for both title-abstract and full-text screening can be found in Appendix 2.



## Data extraction and items

Data was extracted by AF, with JM performing a check to ensure accurate extraction on five consecutive papers, which was achieved after eight papers. Authors were contacted when data was partially incomplete (e.g. means but no standard deviations), appeared to contain errors, or potentially duplicated data from another study. Relevant data included type of pain condition, participant details,  measures of pain and mood and scores on the cognitive screening test (mean and standard deviation, or median and inter-quartile range), as well as whether the test was key or incidental to the study (e.g. a baseline measure). Where data was provide on multiple outcomes (cognitive screens) all outcomes were extracted. A data dictionary can be found in Appendix 3.

 

## Quality assessment tool

Study quality was assessed using the JBI Critical Appraisal Checklist for Analytical Cross Sectional Studies [@Briggs2017]. The checklist includes items that required some adaptation for relevance in this review, which was supplied to both reviewers as supplementary guidance reproduced in Appendix 3. Briefly, the items concerned: 1) criteria for sample inclusion; 2) describing subjects and demographics; 3) measurement of pain with a validated tool; 4) not used as deemed redundant for this SR; 5) reporting confounding factors of age, education, mood and medication usage; 6) matching or controlling for age and education^[it was deemed unrealistic to expect mood and medication to be extricated from pain in most clinical samples; more detail in discussion section.]; 7) reporting  information about screen administration; 8) appropriate screen data available without floor effects introduced by a cognitive screen cut-off.
 No overall risk of bias score is produced using this tool. Reviewers AF and JM independently reviewed quality for five studies  before meeting to address discrepancies. This meeting identified overlap on how the criteria were being applied and led to clarification within the supplementary guidance, after which further quality assessment was completed by AF.

## Statistical analyses  - effect measures and synthesis

Cognitive screen means and standard deviations were used to compute standardised mean differences (SMD) in the form of Hedges' g using the approach described by @Hedges1985. Scores reflect the size of effect due to pain status with larger positive scores reflecting greater impairment in the chronic pain group. Where data was presented as median and inter-quartile range, this was converted into estimated mean and standard deviation with the *estmeansd* R package using the Box–Cox method described in @McGrath2020. 

Analyses were conducted using the *metafor* package [@Viechtbauer2010] with a random-effects model using restricted maximum-likelihood (REML) estimation to measure between-study variance and producing a Wald-type confidence interval. Individual and aggregated effect sizes were visualised using forest plots. A multi-level meta-analytic approach was taken as for a number of studies  more than one comparison of cognitive screen scores fit  review criteria (due either to multiple chronic pain groups compared to one control group or two cognitive screens administered across participants). In most instances this led to generating multiple standardised mean differences per study^[In one case, it was judged that groups could be more appropriately merged to produce a single standardised mean difference.]. This produces interdependency between outcomes best addressed by a multi-level approach to pooling data. This involved a correlated and hierarchical effects model which drew on a covariance matrix estimating these interdependencies, incorporating information about the relationships between cognitive screen scores.  We also explored whether results differed when employing robust variance estimation methods to further account for non-independence. Details are provided in Appendix 4. Two types of sub-group analysis were considered: those focusing on data from a single cognitive screening measure, and those involving data from a single pain condition. These analyses were attempted for situations where five or more comparisons were available. Analysis code is available at https://osf.io/hwa4j/.

## Reporting bias assessment

A funnel plot was produced to investigate whether results may be missing in a non-random fashion due to reporting bias. In many studies our measure of interest (cognitive screen) was incidental to the wider motive for the research (e.g. merely to report sample characteristics). The funnel plot excluded these studies to consider only those where the findings hinged on the cognitive screening data, to identify whether there has been a systematic under-reporting of non-significant findings.


# Results

## Study selection
 A total of 3505 records were identified, 485 of which were initially identified as duplicates. Following two sifting stages (an additional step was made when accessing full-texts to dispose of a large number of conference abstracts), 140 were examined in full text. This led to 45 studies that appeared to meet inclusion criteria, but we excluded one study [@Han2013] that did explicitly report a chronic pain group but using a minimum duration of one month of pain (rather than three), with no diagnostic information or mean duration reported to verify that this would constitute chronic pain by our criteria.  Seven further articles were identified by hand-searching prior to and following the search; these involved studies discovered in review papers (e.g. of arthritis) that either did not refer to pain in key words/title/abstract or did not make explicit that it contained cognitive screening data. This lead to 51 studies in total. Figure 1 depicts this as a flow diagram.






## Study characteristics



``` {r, info, echo = F} 
#### variable creation for below text

su_scr <- dfm_mod %>%
  group_by(cognitive_name)%>%
  count()

numbs <- ((su_scr[1:5,2])) # still a weird object so use [[]] to subset

names <- levels(dfm_mod$cognitive_name)

```

The search process led to the extraction of `r length(unique(dfm2$unique_id))` effect size estimates from `r length(unique(dfm2$study_id))` studies. Data from `r sum(dfm2$n_treat)` people experiencing chronic pain and `r sum(dfm2$n_cont) ` pain-free controls was extracted.  There were `r numbs[[1,1]]` comparisons involving the `r names[1]`, `r numbs[[2,1]] ` for the `r names[2]`, `r numbs[[3,1]]` for the `r names[3]`, `r numbs[[4,1]]` for the `r names[4]`, and `r numbs[[5,1]]` for the `r names[5]`. In `r sum(dfm_mod$cognitive_focus=="key")` comparisons, the screen was part of the study focus and `r sum(dfm_mod$cognitive_focus=="incidental")` where the screen was used merely to inform the description of samples. Table 1 presents summary information on these studies.




## Risk of bias  in studies

```{r qualityfails}
inclusiondesc <- quality_scores_vals %>% count(J1)
sampledescribe <- quality_scores_vals %>% count(J1)
painrep <- quality_scores_vals %>% count(J3)
confoundrep <- quality_scores_vals %>% count(J5)
confoundcont <- quality_scores_vals %>% count(J6)
cogreport <- quality_scores_vals %>% count(J7)
cogcutoff <- quality_scores_vals %>% count(J8)
toggles <- quality_scores_vals %>% count(toggle)
```

Only three comparisons (from two studies) met every JBI criterion. The majority of comparisons (k = `r inclusiondesc[3,2]`) did adequately describe sample details and inclusion criteria as per criterion 1 and 2. However a minority of comparisons met criteria 3 ( k = `r painrep[3,2]`/`r sum(painrep[,2])`) or 5 (k =`r confoundrep[3,2]`) due respectively to lack of pain measurement in both control and pain groups and non-reporting of mood, education, age or medication information. Half of comparisons (k = `r confoundcont[1,2]`)  failed to control for education and age (criterion 6) and around half  (k = `r cogreport[1,2]`)  failed to provide information about administration of the cognitive screen (criterion 7). Finally `r cogcutoff[1,2]` studies showed bias on criterion 8, involving a floor on screening scores due to exclusion of low-scoring participants^[In most cases the cut-off employed (e.g. MMSE 24 or 28) fell within a 68% coverage interval of scores observed within our datasets; we exempted two cases where the threshold was very low and well outside this (e.g. MMSE 10 or 14) which removed only the severely cognitively impaired as part of exclusion criteria.]. 

As the appraisal tool does not provide guidance as to how to categorize studies as high or low risk of bias, we considered the implications of failing to meet each criterion and determined that for this SR, studies with low risk of bias were those meeting these key criteria: 

- adequate exclusion criteria (J1)
- controlled or matched for age and education (J6)
- did not employ a cut-off that prevented detection of poor performance (J8)

This resulted in  `r toggles[2,2]` comparisons with a low risk of bias.

```{r moodsum}

mooddiffs <- aim2 %>%
  group_by(q6_mood_confound) %>%
 count()

mooddiffs_x <- mooddiffs[1:4,2]

```

As criterion 6 does not evaluate controlling for mood, this was evaluated separately: mood disturbance was significantly higher for the pain group in `r mooddiffs_x[[2,1]]` of the comparisons, with `r mooddiffs_x[[3,1]] +  mooddiffs_x[[4,1]]` not reporting mood and only `r mooddiffs_x[[1,1]]` reporting similar levels of mood. 





## Results of syntheses

For the meta-analytic calculations we first employed the random effects multi-level meta-analytic model using the full dataset. The pooled SMD estimate (with a positive effect denoting degree of impairment in pain groups) was `r format(round(main_model_output $pred, 2), nsmall = 2)` with the 95% confidence interval ranging from `r format(round(main_model_output $ci.lb, 2), nsmall = 2)` to `r format(round(main_model_output $ci.ub, 2), nsmall = 2)`. This describes the range within which we expect the average effect size to fall. A comparison of dataset heterogeneity against within-comparison variances suggests that the comparisons reflect different effects (Cochran's Q = `r round(che.model$QE,2)`, p < 0.0001). The estimated variance components were 	$\tau_{Level3}^2$ = `r round(che.model$sigma2[1],3)` and $\tau_{Level2}^2$ = `r round(che.model$sigma2[2],3)`, meaning that between-study variation accounts for `r model.i2$results[3,2]`% of the total variation, whereas `r model.i2$results[2,2]`% is due to variability between multiple comparisons within a single study. Figure 3 depicts the SMDs for each comparison. 

#### Subgroup analysis
A simple random effects model produced a higher pooled SMD estimate, suggesting that effect non-independence was evident, thus justifying the multi-level approach where clusters existed. A sensitivity analysis using robust variance estimators produced almost identical outputs to the standard multi-level method. A further sensitivity analysis focused on comparisons with a low risk of bias, and from these comparisons sub-analyses were conducted with i) MMSE screen only ii) MoCA screen only iii) arthritis pain condition only, employing the multi-level approach when the data-set contained clusters and iv) a stronger test of effects within low-risk of bias situations with only comparisons where groups had similar levels of mood. Sub-analyses are presented for the other major pain conditions (fibromyalgia, headache and musculoskeletal conditions), but note that these contain high- and low-risk of bias comparisons as too few comparisons were otherwise available. For comparison an analysis with high risk of bias studies is included in the table. Study heterogeneity remained fairly high across these analyses except for the one focused on comparable mood scores, where  $I^2$ should be interpreted carefully as study numbers are small [@Hippel2015]. These are reported in Table 2.



## Reporting biases



``` {r eggrs}
 dfm_key<- dfm_mod %>%
    filter(cognitive_focus == "key")
res_key <- rma(yi, vi, data=dfm_key)
eggers<- regtest(res_key, model="rma",  ret.fit=FALSE, digits = 2)

test <- rma(yi,vi, data= dfm_mod)

```


Fig 4 shows a funnel plot including studies where the cognitive screen was key to the study purpose. This uses the trim-and-fill method [@Duval2000] to interpolate study effect sizes that would be expected from the extracted results, suggesting where low-powered and non-significant effects may be absent from the distribution. The plot suggests that no additional low-powered, non-significant effects would be expected given the observed distribution. The Egger test was conducted which seeks to identify a relationship between effect size and precision of estimate that may indicate systematic publication bias. The estimated coefficient was `r round(eggers$zval,2)`, p = `r  round(eggers$pval,2)`, meaning the test cannot reject the null of no such relationship.

# Discussion

## Summary of findings

There appears to be considerable evidence for chronic pain being associated with lower scores on cognitive screens. For every analysis and sub-analysis, the 95% coverage interval did not include zero, suggesting that across diverse groups experiencing chronic pain, cognitive screen performance is lower than for control groups - even when low mood, which frequently co-occurs with pain, is similar across groups. However, the high levels of heterogeneity suggest that the sources of this effect may be manifold. Sub-analyses on the two most represented screens (MMSE and MoCA) saw some reduction of heterogeneity with larger effect sizes for comparisons involving the MoCA. The overall estimate for low risk of bias studies was `r round(predict(che.hi)$pred,3)` (95% CI `r round(che.hi$ci.lb, 3)` - `r round(che.hi$ci.ub, 3)`).

Different pain conditions yielded slightly different pooled effects. 
The highest overall effect was for chronic headache/migraine sufferers; however this effect had the 95% coverage interval which came closest to including zero. Fibromyalgia studies followed a similar pattern: a large point estimates with a wide confidence interval. The MSK group saw smaller effect sizes within again a wide confidence interval. Note that these analyses included studies at a high risk of bias, which as a whole produced higher point estimates and wider coverage intervals than studies with a low risk of bias. Studies (limited to low risk of bias) on arthritis  demonstrated more consistency and an intermediate point estimate. We also note that the two studies [@Karp2008; @Terassi2021] explicitly limited to older adults (with low risk of bias) reported two of the three^[with @Torkamani2015.] smallest SMDs.


## Comparison with previous research

### Pain conditions

For people living with arthritis, the current MA found similar effect sizes to those reported in @Pankowski2022. The current arthritis sub-analysis  excluded two studies from the Pankowski review due to rated high risk of bias - the MMSE study by @Petersen2015 that excluded participants based on MMSE scores, and the MoCA study by @Kotb2019 which lacked information about levels of education. The current sub-analysis  included two further arthritis studies judged to have low risk of bias that were published after the prior review completed its searches.

Inflammatory diseases are increasingly understood to have neurological implications, meaning that patients with these conditions may score differently on cognitive screens because of pain *and* the direct action of the disease such as premature immunosenescence [@Petersen2015]. There is evidence, for instance, that patients with rheumatoid arthritis attain poorer MoCA scores than controls with similar levels of bodily pain [@Kim2018]. Similarly, brain changes are noted with chronic headache, for instance medication-overuse headache with increased white matter hyperintensity [@Xiang2021] and changes in functional connectivity in the neostriatum [@Chen2016]; however note neurological changes need not result in an impact on cognition. Previous research has reported patients with fibromyalgia to have higher prevalence of cognitive deficit based on screen performance compared to other forms of pain [e.g. neuropathic or mixed pain, @RodriguezAndreu2009] although score ranges do not differ drastically.  In this MA the effects for musculoskeletal pain were lower than for other pain conditions. This may relate to severity of pain, which was not analysed in this study. @Chen2011 reported that patients with self-reported musculoskeletal chronic pain who scored in the upper quartile on the Brief Pain Inventory Severity Scale were twice as likely as those in the lower quartile to obtain an MMSE score below 24 (18.5% versus 9.7%). However, @Bosley2004 found no difference in MMSE scores between groups with significantly different pain intensity scores (MPQ-SF).

### Cognitive scores

The larger effect sizes for the MoCA versus MMSE mirror that found by @Pankowski2022. It is possible that there are true differential effects across these screens, driven by differences such as the lack of executive function assessment by the MMSE [@NieuwenhuisMark2010], as this is a core domain influenced by pain. However the differences we observe may instead reflect the wider heterogeneity across studies, eg due to included pain conditions.

## Limitations of evidence

The quality assessment found very few studies met every JBI criteria. In some cases, study design and decisions may not reflect poor quality *per se*; for instance it may be appropriate to screen out lower scoring participants on screens for some study objectives. The infrequency of conducting pain measurements on control participants is understandable but prevents objective comparison of pain levels that could add more confidence to findings. 

Most studies were uncontrolled for mood and medication differences between pain and control groups, which may reflect the realities of typical clinical samples: medication for the condition in question, and mood as understood to be a prevalent and co-occurring symptom with pain, forming part of a symptom cluster [@Davis2016]. @Rock2014 report small to moderate effects of depression on cognition; however a sub-analysis in the current meta-analysis involving  groups with similar mood scores produced effect estimates broadly in line with that for all low risk of bias comparisons. Despite some statistical differences in mood scores by group, the SR dataset may not have contained a preponderance of individuals who would meet caseness for depression (and indeed some studies explicitly excluded participants on the basis of such diagnoses). Meanwhile, commonly prescribed medications for rheumatoid arthritis have been associated with cognitive impairment [on methotrexate, see @Pamuk2013; on glucocorticoid therapy, see @Coluccia2008]. However a study by @Gogol2014a looked at the impact of opioidal medication on MMSE performance and reported no effect. 
 
Given that cognitive performance is known to be influenced by age and education, failure to match for these measures clearly introduces a risk of bias (although some studies may still mitigate this for their primary research question by controlling within a subsequent analysis of interest). 

If pain were under-reported by people with cognitive impairment this would introduce systematic error into the findings. There is evidence of such under-reporting for people with a dementia diagnosis [reviewed by @Scherder2005]; however the current review excluded studies on that diagnosis or with similar neuropsychological impairments. Other research shows that less pronounced cognitive deficits are not associated with changes in pain perception or reporting [@Kunz2009; @Docking2014].
 
## Limitations of review processes

 This review focused on studies comparing pain and pain-free groups, meaning that it did not include longitudinal studies examining change over time or due to interventions, which could give insights into how cognitive screen performance follows the course of pain condition/fluctuation in pain experience. There was also insufficient specification of pain duration across studies to allow controlling for or investigating this factor, which is likely to be of interest given the parallels between chronic pain and neurodegenerative disease [@Apkarian2006]. 

This SR encompasses a range of diagnostic groups and samples without diagnosis (based on self-ratings), which is likely to contribute to the heterogeneity of the findings. We felt it important to represent the range of forms of chronic pain that could arise for clinicians in the assessment of dementia, and have conducted sub-analyses by condition where there was available data.


## Constraints on generality

This review chose to limit its cognitive screens to a fairly narrow list^[In preparation for this review, the first author collated a list of over 100 measures described across systematic reviews of cognitive screens.], to align with clinical usage. This list was made by UK clinicians and that this may limit generalizability. Moreover, besides the MoCA and MMSE, there were very few studies extracted for the other cognitive screens, meaning that this review heavily focuses on just two measures. However, the studies included come from a range of countries including multiple from the Middle East, South America and Asia, avoiding a focus merely on European and North American samples.



## Meaning and implications
Given the association of effects of between half and one standard deviation poorer performance in chronic-pain experiencing participants, clinicians may wish to consider this when administering cognitive screens. Based on point estimate from the low-risk of bias analysis (`r round(predict(che.hi)$pred,1)`), with normative data suggesting the major measures have standard deviations of around three or four [MMSE 3 - @Tombaugh1996 ; MoCA 4 - @Rossetti2011] into raw scores using normative data, we might plausibly see decrements of `r round(predict(che.hi)$pred*3,1)` - `r round(predict(che.hi)$pred*4,1)` points. Crucial to consider however is whether for at least some conditions involving pain, cognitive impairment may be a marker for later severe decline [@Tian2023]. Future research should explore screen performance by sub-domains, to identify if there are certain areas where pain impacts and others where it does not affect results.

```{r tempry}
# Normative data suggests that in groups with no cognitive impairment, MMSE SD is around three points [@Tombaugh1996] meaning that a typical decrement of `r round(predict(che.hi)$pred*3,1)` [95% CI `r round(predict(che.hi)$ci.lb*3,1)` - `r round(predict(che.hi)$ci.ub*3,1)`] points is a plausible estimate. The larger effects for the MoCA tool would [based on norms in @Rossetti2011 suggesting SD of 4] equate to `r round(predict(simple.moca)$pred*4,1)`  [95% CI `r round(predict(simple.moca)$ci.lb*4,1)` - `r round(predict(simple.moca)$ci.ub*4,1)`] points.
```
 

# Declaration of interest

None.

# References

<div id="refs"></div>




``` {r, overviewtab, echo = F}
### Study characteristics table - keep here



overview <- read.xlsx(here("data", "study_overview_sheet.xlsx"))
overview <- overview %>%
  mutate(biblio_comp = paste0("@",biblio))

pres_overview <- select(overview, study_id, biblio_comp, country)

pres_dat <- dfm_e %>%
  select(study_id, sample_treat, n_treat, age_mean_treat, age_sd_treat,  n_cont, matching_education, matching_age, cognitive_name,cognitive_variant )

test <- left_join(pres_dat,pres_overview, by="study_id")

test <- test %>%
  mutate(age_mean_treat = round(age_mean_treat, 2),
         n_treat = round(n_treat,0),
         n_cont = round(n_cont,0),
         age_sd_treat = round(age_sd_treat, 2),
         age_sum_treat = paste0(age_mean_treat," (",age_sd_treat,")"),
         # age_sum_treat = if_else(is.na(age_sum_treat),"NR", age_sum_treat),
         age_sum_treat = case_when(
           study_id ==   "LI_120" ~ "higher for patients", 
           age_sum_treat == "NR"  ~ "NR",
           matching_age == "no sig diff" ~ age_sum_treat,
           matching_age == "controls older" ~ paste0(age_sum_treat, ('^†^')),
           matching_age == "patients older" ~ paste0(age_sum_treat, ('^\\*^')
                                                )
         ),
         cognitive_name = ifelse(cognitive_name %in% c("ACE"),
                                 paste0(cognitive_name,"-", cognitive_variant),paste0(cognitive_name))
  )


tab1 <- test %>%
  select(!c(age_mean_treat,age_sd_treat,cognitive_variant, matching_age, study_id, n_cont)) %>%
  arrange(biblio_comp) %>%
  relocate(biblio_comp, country, sample_treat, n_treat, matching_education, age_sum_treat, cognitive_name) 
 

apa_table(tab1
          , landscape = TRUE
          , longtable = TRUE
          , col.names = c("Authors","Country","Pain group","Patient group *n*","Education","Patient age - Mean (SD)","Screen")
          ,  caption = "Characteristics of studies."
          , note = "MSK = Musculo-skeletal condition. SLE = Systemic Lupus Erythematosus. Test abbreviations: ACE = Addenbrooke's Cognitive Examination (R = Revised, III = 3rd edition). HVLT = Hopkins Verbal Learning Test. MMSE = Mini-Mental State Examination. MoCA = Montreal Cognitive Assessment. TYM = Test Your Memory test. \U2020 denotes patients significantly older, \U002A\ controls significantly older. NB Li et all reported significant age differences but age data did not allow extraction of a mean and standard deviation.")
```

```{r, regressiontab}

ma_n <-tibble(totals = c(
         as.integer(sum(dfm_mod$n_treat) + sum(dfm_mod$n_cont)),
         as.integer(sum(dfm_mod$n_treat) + sum(dfm_mod$n_cont)), 
         as.integer(sum(dfm_hi$n_treat) + sum(dfm_hi$n_cont)),
         as.integer(sum(dfm_hi$n_treat) + sum(dfm_hi$n_cont)),
         as.integer(sum(dfm_lo$n_treat) + sum(dfm_lo$n_cont)),
         as.integer(sum(dfm_lo$n_treat) + sum(dfm_lo$n_cont)),
         as.integer(sum(dfm_mmse$n_treat) + sum(dfm_mmse$n_cont)),
         as.integer(sum(dfm_mmse$n_treat) + sum(dfm_mmse$n_cont)),
         as.integer(sum(dfm_arthritis$n_treat) + sum(dfm_arthritis$n_cont)),
         as.integer(sum(dfm_arthritis$n_treat) + sum(dfm_arthritis$n_cont)),
         as.integer(sum(dfm_fm_full$n_treat) + sum(dfm_fm_full$n_cont)),
         as.integer(sum(dfm_fm_full$n_treat) + sum(dfm_fm_full$n_cont)),
         as.integer(sum(dfm_msk_full$n_treat) + sum(dfm_msk_full$n_cont)),
         as.integer(sum(dfm_msk_full$n_treat) + sum(dfm_msk_full$n_cont)),
         as.integer(sum(dfm_head_full$n_treat) + sum(dfm_head_full$n_cont)),
         as.integer(sum(dfm_head_full$n_treat) + sum(dfm_head_full$n_cont)),
         as.integer(sum(dfm_moca$n_treat) + sum(dfm_moca$n_cont)),
         as.integer(sum(dfm_depfree$n_treat) + sum(dfm_depfree$n_cont))
         ))

total_table_n <- cbind(total_table, ma_n)

tab2 <- total_table_n %>%
    select(-order,-justes) %>%
  relocate(type, method, n, totals)


apa_table(tab2
          , landscape = TRUE
          , longtable = TRUE
          , col.names = c("Dataset","Meta-analytic method","*k*","*n*","SMD [95% confidence interval]", "p-value","*I^2^*","Within- cluster *I^2^*","Between- cluster *I^2^*")
          ,  caption = "Chronic pain status associations with cognitive screen performance."
          , note = "k = number of studies, n = number of data points, SMD = standardised mean difference, I^2^ = heterogeneity statistic, decomposed into two levels (within each study and between each study) for the multi-level approach, RE = Random Effects, CHE = Correlated & Hierarchical Effects, MMSE = Mini-mental State Examination, MSK = musculoskeletal condition, MoCA = Montreal Cognitive Assessment")





```

(ref:prism-label) PRISMA flow diagram.

(ref:risk-label) Risk of bias plot. Asterisks denote those determined to have low risk of bias. Column numbers are JBI items. Criterion 4 not utilised due to redundancy with other items in this context.

(ref:forest-label) Forest plot. Depicts individual effects and pooled effects based on multi-level CHE model.

(ref:fun-label) Funnel Plot (focused studies).

```{r, prisming, fig.cap = "(ref:risk-label)", echo = F}
knitr::include_graphics("../graphs/flowchart.jpg")
```

```{r, risking, fig.cap = "(ref:risk-label)", echo = F}
knitr::include_graphics("../graphs/quality_simp.png")
```

```{r, foresting, fig.cap = "(ref:forest-label)", echo = F}
knitr::include_graphics("../graphs/Forest_rich.png")
```

``` {r, funnelling, fig.cap = "(ref:fun-label)", echo = F}

res_tri <- trimfill(res_key)
funnel(res_tri, legend= FALSE, main = "")  #level=c(90, 95), shade=c("white", "gray55"), 

#Figure 3. Forest plot. Depicts individual effects and pooled effects based on multi-level CHE model 

```

``` {r, funnelmetaviz, echo = F, include = F}
library(metaviz)
 dfm_key<- dfm_mod %>%
    filter(cognitive_focus == "key")
 res_key <- rma(yi, vi, data=dfm_key)
 funnelinf(res_key, n = 1, 
          egger = TRUE, 
          sig_contours = FALSE,
          trim_and_fill = TRUE 
          )
 
```



